{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DlLCK6UgrrpW"
   },
   "source": [
    "# Project 01 - Classify the 10 digits using MNIST data\n",
    "\n",
    "### Group Members\n",
    "- Koushik Gadeela\n",
    "- Rindhuja Johnson\n",
    "- Rohith Challa\n",
    "- Rutuja Karad\n",
    "\n",
    "You are required to implement the convolutional neural network using purely numpy to classify the digits. All the techniques and codes will be covered in the next 2 weeks.\n",
    "\n",
    "## Due date\n",
    "\n",
    "Nov 7th, 2023\n",
    "\n",
    "## Extra Credits\n",
    "If you want to speed up the training using GPU, you will need to write C++ code and wrap it using pybind11 and call it in your python code. This will give you 10 extra credits for this project.\n",
    "\n",
    "## Rubics\n",
    "1. If you code can run at my side and include the basic training cycle, you will get 40% of the total score;\n",
    "2. Your meeting notes will be 20% of the project;\n",
    "3. Your implementation of fully connected layers (forward and backpropagation pass) ----- 20%;\n",
    "4. Your implementation of Convolution layer (forward and backpropagation pass) ----- 20%\n",
    "\n",
    "## Data Description\n",
    "[MNIST](https://en.wikipedia.org/wiki/MNIST_database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Extraction and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ohha6cR7ufYF"
   },
   "outputs": [],
   "source": [
    "# load the mnist data\n",
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ChXaGZKCulwM",
    "outputId": "a0f4d5ca-0bdb-4186-e16c-8f308bbf874a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Lib\\site-packages\\sklearn\\datasets\\_openml.py:1022: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "mnist_28 = fetch_openml('mnist_784') #fetch the mnist_784 dataset using the fetch_openml API and store it in a variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ar7Bie1YeZqR",
    "outputId": "ba7fd8fb-9cdd-464e-b415-773d841a9ebf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data':        pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
       " 0         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       " 1         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       " 2         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       " 3         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       " 4         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       " ...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       " 69995     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       " 69996     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       " 69997     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       " 69998     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       " 69999     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       " \n",
       "        pixel10  ...  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       " 0          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       " 1          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       " 2          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       " 3          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       " 4          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       " ...        ...  ...       ...       ...       ...       ...       ...   \n",
       " 69995      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       " 69996      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       " 69997      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       " 69998      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       " 69999      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       " \n",
       "        pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       " 0           0.0       0.0       0.0       0.0       0.0  \n",
       " 1           0.0       0.0       0.0       0.0       0.0  \n",
       " 2           0.0       0.0       0.0       0.0       0.0  \n",
       " 3           0.0       0.0       0.0       0.0       0.0  \n",
       " 4           0.0       0.0       0.0       0.0       0.0  \n",
       " ...         ...       ...       ...       ...       ...  \n",
       " 69995       0.0       0.0       0.0       0.0       0.0  \n",
       " 69996       0.0       0.0       0.0       0.0       0.0  \n",
       " 69997       0.0       0.0       0.0       0.0       0.0  \n",
       " 69998       0.0       0.0       0.0       0.0       0.0  \n",
       " 69999       0.0       0.0       0.0       0.0       0.0  \n",
       " \n",
       " [70000 rows x 784 columns],\n",
       " 'target': 0        5\n",
       " 1        0\n",
       " 2        4\n",
       " 3        1\n",
       " 4        9\n",
       "         ..\n",
       " 69995    2\n",
       " 69996    3\n",
       " 69997    4\n",
       " 69998    5\n",
       " 69999    6\n",
       " Name: class, Length: 70000, dtype: category\n",
       " Categories (10, object): ['0', '1', '2', '3', ..., '6', '7', '8', '9'],\n",
       " 'frame':        pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
       " 0         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       " 1         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       " 2         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       " 3         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       " 4         0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       " ...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       " 69995     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       " 69996     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       " 69997     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       " 69998     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       " 69999     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       " \n",
       "        pixel10  ...  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       " 0          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       " 1          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       " 2          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       " 3          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       " 4          0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       " ...        ...  ...       ...       ...       ...       ...       ...   \n",
       " 69995      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       " 69996      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       " 69997      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       " 69998      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       " 69999      0.0  ...       0.0       0.0       0.0       0.0       0.0   \n",
       " \n",
       "        pixel781  pixel782  pixel783  pixel784  class  \n",
       " 0           0.0       0.0       0.0       0.0      5  \n",
       " 1           0.0       0.0       0.0       0.0      0  \n",
       " 2           0.0       0.0       0.0       0.0      4  \n",
       " 3           0.0       0.0       0.0       0.0      1  \n",
       " 4           0.0       0.0       0.0       0.0      9  \n",
       " ...         ...       ...       ...       ...    ...  \n",
       " 69995       0.0       0.0       0.0       0.0      2  \n",
       " 69996       0.0       0.0       0.0       0.0      3  \n",
       " 69997       0.0       0.0       0.0       0.0      4  \n",
       " 69998       0.0       0.0       0.0       0.0      5  \n",
       " 69999       0.0       0.0       0.0       0.0      6  \n",
       " \n",
       " [70000 rows x 785 columns],\n",
       " 'categories': None,\n",
       " 'feature_names': ['pixel1',\n",
       "  'pixel2',\n",
       "  'pixel3',\n",
       "  'pixel4',\n",
       "  'pixel5',\n",
       "  'pixel6',\n",
       "  'pixel7',\n",
       "  'pixel8',\n",
       "  'pixel9',\n",
       "  'pixel10',\n",
       "  'pixel11',\n",
       "  'pixel12',\n",
       "  'pixel13',\n",
       "  'pixel14',\n",
       "  'pixel15',\n",
       "  'pixel16',\n",
       "  'pixel17',\n",
       "  'pixel18',\n",
       "  'pixel19',\n",
       "  'pixel20',\n",
       "  'pixel21',\n",
       "  'pixel22',\n",
       "  'pixel23',\n",
       "  'pixel24',\n",
       "  'pixel25',\n",
       "  'pixel26',\n",
       "  'pixel27',\n",
       "  'pixel28',\n",
       "  'pixel29',\n",
       "  'pixel30',\n",
       "  'pixel31',\n",
       "  'pixel32',\n",
       "  'pixel33',\n",
       "  'pixel34',\n",
       "  'pixel35',\n",
       "  'pixel36',\n",
       "  'pixel37',\n",
       "  'pixel38',\n",
       "  'pixel39',\n",
       "  'pixel40',\n",
       "  'pixel41',\n",
       "  'pixel42',\n",
       "  'pixel43',\n",
       "  'pixel44',\n",
       "  'pixel45',\n",
       "  'pixel46',\n",
       "  'pixel47',\n",
       "  'pixel48',\n",
       "  'pixel49',\n",
       "  'pixel50',\n",
       "  'pixel51',\n",
       "  'pixel52',\n",
       "  'pixel53',\n",
       "  'pixel54',\n",
       "  'pixel55',\n",
       "  'pixel56',\n",
       "  'pixel57',\n",
       "  'pixel58',\n",
       "  'pixel59',\n",
       "  'pixel60',\n",
       "  'pixel61',\n",
       "  'pixel62',\n",
       "  'pixel63',\n",
       "  'pixel64',\n",
       "  'pixel65',\n",
       "  'pixel66',\n",
       "  'pixel67',\n",
       "  'pixel68',\n",
       "  'pixel69',\n",
       "  'pixel70',\n",
       "  'pixel71',\n",
       "  'pixel72',\n",
       "  'pixel73',\n",
       "  'pixel74',\n",
       "  'pixel75',\n",
       "  'pixel76',\n",
       "  'pixel77',\n",
       "  'pixel78',\n",
       "  'pixel79',\n",
       "  'pixel80',\n",
       "  'pixel81',\n",
       "  'pixel82',\n",
       "  'pixel83',\n",
       "  'pixel84',\n",
       "  'pixel85',\n",
       "  'pixel86',\n",
       "  'pixel87',\n",
       "  'pixel88',\n",
       "  'pixel89',\n",
       "  'pixel90',\n",
       "  'pixel91',\n",
       "  'pixel92',\n",
       "  'pixel93',\n",
       "  'pixel94',\n",
       "  'pixel95',\n",
       "  'pixel96',\n",
       "  'pixel97',\n",
       "  'pixel98',\n",
       "  'pixel99',\n",
       "  'pixel100',\n",
       "  'pixel101',\n",
       "  'pixel102',\n",
       "  'pixel103',\n",
       "  'pixel104',\n",
       "  'pixel105',\n",
       "  'pixel106',\n",
       "  'pixel107',\n",
       "  'pixel108',\n",
       "  'pixel109',\n",
       "  'pixel110',\n",
       "  'pixel111',\n",
       "  'pixel112',\n",
       "  'pixel113',\n",
       "  'pixel114',\n",
       "  'pixel115',\n",
       "  'pixel116',\n",
       "  'pixel117',\n",
       "  'pixel118',\n",
       "  'pixel119',\n",
       "  'pixel120',\n",
       "  'pixel121',\n",
       "  'pixel122',\n",
       "  'pixel123',\n",
       "  'pixel124',\n",
       "  'pixel125',\n",
       "  'pixel126',\n",
       "  'pixel127',\n",
       "  'pixel128',\n",
       "  'pixel129',\n",
       "  'pixel130',\n",
       "  'pixel131',\n",
       "  'pixel132',\n",
       "  'pixel133',\n",
       "  'pixel134',\n",
       "  'pixel135',\n",
       "  'pixel136',\n",
       "  'pixel137',\n",
       "  'pixel138',\n",
       "  'pixel139',\n",
       "  'pixel140',\n",
       "  'pixel141',\n",
       "  'pixel142',\n",
       "  'pixel143',\n",
       "  'pixel144',\n",
       "  'pixel145',\n",
       "  'pixel146',\n",
       "  'pixel147',\n",
       "  'pixel148',\n",
       "  'pixel149',\n",
       "  'pixel150',\n",
       "  'pixel151',\n",
       "  'pixel152',\n",
       "  'pixel153',\n",
       "  'pixel154',\n",
       "  'pixel155',\n",
       "  'pixel156',\n",
       "  'pixel157',\n",
       "  'pixel158',\n",
       "  'pixel159',\n",
       "  'pixel160',\n",
       "  'pixel161',\n",
       "  'pixel162',\n",
       "  'pixel163',\n",
       "  'pixel164',\n",
       "  'pixel165',\n",
       "  'pixel166',\n",
       "  'pixel167',\n",
       "  'pixel168',\n",
       "  'pixel169',\n",
       "  'pixel170',\n",
       "  'pixel171',\n",
       "  'pixel172',\n",
       "  'pixel173',\n",
       "  'pixel174',\n",
       "  'pixel175',\n",
       "  'pixel176',\n",
       "  'pixel177',\n",
       "  'pixel178',\n",
       "  'pixel179',\n",
       "  'pixel180',\n",
       "  'pixel181',\n",
       "  'pixel182',\n",
       "  'pixel183',\n",
       "  'pixel184',\n",
       "  'pixel185',\n",
       "  'pixel186',\n",
       "  'pixel187',\n",
       "  'pixel188',\n",
       "  'pixel189',\n",
       "  'pixel190',\n",
       "  'pixel191',\n",
       "  'pixel192',\n",
       "  'pixel193',\n",
       "  'pixel194',\n",
       "  'pixel195',\n",
       "  'pixel196',\n",
       "  'pixel197',\n",
       "  'pixel198',\n",
       "  'pixel199',\n",
       "  'pixel200',\n",
       "  'pixel201',\n",
       "  'pixel202',\n",
       "  'pixel203',\n",
       "  'pixel204',\n",
       "  'pixel205',\n",
       "  'pixel206',\n",
       "  'pixel207',\n",
       "  'pixel208',\n",
       "  'pixel209',\n",
       "  'pixel210',\n",
       "  'pixel211',\n",
       "  'pixel212',\n",
       "  'pixel213',\n",
       "  'pixel214',\n",
       "  'pixel215',\n",
       "  'pixel216',\n",
       "  'pixel217',\n",
       "  'pixel218',\n",
       "  'pixel219',\n",
       "  'pixel220',\n",
       "  'pixel221',\n",
       "  'pixel222',\n",
       "  'pixel223',\n",
       "  'pixel224',\n",
       "  'pixel225',\n",
       "  'pixel226',\n",
       "  'pixel227',\n",
       "  'pixel228',\n",
       "  'pixel229',\n",
       "  'pixel230',\n",
       "  'pixel231',\n",
       "  'pixel232',\n",
       "  'pixel233',\n",
       "  'pixel234',\n",
       "  'pixel235',\n",
       "  'pixel236',\n",
       "  'pixel237',\n",
       "  'pixel238',\n",
       "  'pixel239',\n",
       "  'pixel240',\n",
       "  'pixel241',\n",
       "  'pixel242',\n",
       "  'pixel243',\n",
       "  'pixel244',\n",
       "  'pixel245',\n",
       "  'pixel246',\n",
       "  'pixel247',\n",
       "  'pixel248',\n",
       "  'pixel249',\n",
       "  'pixel250',\n",
       "  'pixel251',\n",
       "  'pixel252',\n",
       "  'pixel253',\n",
       "  'pixel254',\n",
       "  'pixel255',\n",
       "  'pixel256',\n",
       "  'pixel257',\n",
       "  'pixel258',\n",
       "  'pixel259',\n",
       "  'pixel260',\n",
       "  'pixel261',\n",
       "  'pixel262',\n",
       "  'pixel263',\n",
       "  'pixel264',\n",
       "  'pixel265',\n",
       "  'pixel266',\n",
       "  'pixel267',\n",
       "  'pixel268',\n",
       "  'pixel269',\n",
       "  'pixel270',\n",
       "  'pixel271',\n",
       "  'pixel272',\n",
       "  'pixel273',\n",
       "  'pixel274',\n",
       "  'pixel275',\n",
       "  'pixel276',\n",
       "  'pixel277',\n",
       "  'pixel278',\n",
       "  'pixel279',\n",
       "  'pixel280',\n",
       "  'pixel281',\n",
       "  'pixel282',\n",
       "  'pixel283',\n",
       "  'pixel284',\n",
       "  'pixel285',\n",
       "  'pixel286',\n",
       "  'pixel287',\n",
       "  'pixel288',\n",
       "  'pixel289',\n",
       "  'pixel290',\n",
       "  'pixel291',\n",
       "  'pixel292',\n",
       "  'pixel293',\n",
       "  'pixel294',\n",
       "  'pixel295',\n",
       "  'pixel296',\n",
       "  'pixel297',\n",
       "  'pixel298',\n",
       "  'pixel299',\n",
       "  'pixel300',\n",
       "  'pixel301',\n",
       "  'pixel302',\n",
       "  'pixel303',\n",
       "  'pixel304',\n",
       "  'pixel305',\n",
       "  'pixel306',\n",
       "  'pixel307',\n",
       "  'pixel308',\n",
       "  'pixel309',\n",
       "  'pixel310',\n",
       "  'pixel311',\n",
       "  'pixel312',\n",
       "  'pixel313',\n",
       "  'pixel314',\n",
       "  'pixel315',\n",
       "  'pixel316',\n",
       "  'pixel317',\n",
       "  'pixel318',\n",
       "  'pixel319',\n",
       "  'pixel320',\n",
       "  'pixel321',\n",
       "  'pixel322',\n",
       "  'pixel323',\n",
       "  'pixel324',\n",
       "  'pixel325',\n",
       "  'pixel326',\n",
       "  'pixel327',\n",
       "  'pixel328',\n",
       "  'pixel329',\n",
       "  'pixel330',\n",
       "  'pixel331',\n",
       "  'pixel332',\n",
       "  'pixel333',\n",
       "  'pixel334',\n",
       "  'pixel335',\n",
       "  'pixel336',\n",
       "  'pixel337',\n",
       "  'pixel338',\n",
       "  'pixel339',\n",
       "  'pixel340',\n",
       "  'pixel341',\n",
       "  'pixel342',\n",
       "  'pixel343',\n",
       "  'pixel344',\n",
       "  'pixel345',\n",
       "  'pixel346',\n",
       "  'pixel347',\n",
       "  'pixel348',\n",
       "  'pixel349',\n",
       "  'pixel350',\n",
       "  'pixel351',\n",
       "  'pixel352',\n",
       "  'pixel353',\n",
       "  'pixel354',\n",
       "  'pixel355',\n",
       "  'pixel356',\n",
       "  'pixel357',\n",
       "  'pixel358',\n",
       "  'pixel359',\n",
       "  'pixel360',\n",
       "  'pixel361',\n",
       "  'pixel362',\n",
       "  'pixel363',\n",
       "  'pixel364',\n",
       "  'pixel365',\n",
       "  'pixel366',\n",
       "  'pixel367',\n",
       "  'pixel368',\n",
       "  'pixel369',\n",
       "  'pixel370',\n",
       "  'pixel371',\n",
       "  'pixel372',\n",
       "  'pixel373',\n",
       "  'pixel374',\n",
       "  'pixel375',\n",
       "  'pixel376',\n",
       "  'pixel377',\n",
       "  'pixel378',\n",
       "  'pixel379',\n",
       "  'pixel380',\n",
       "  'pixel381',\n",
       "  'pixel382',\n",
       "  'pixel383',\n",
       "  'pixel384',\n",
       "  'pixel385',\n",
       "  'pixel386',\n",
       "  'pixel387',\n",
       "  'pixel388',\n",
       "  'pixel389',\n",
       "  'pixel390',\n",
       "  'pixel391',\n",
       "  'pixel392',\n",
       "  'pixel393',\n",
       "  'pixel394',\n",
       "  'pixel395',\n",
       "  'pixel396',\n",
       "  'pixel397',\n",
       "  'pixel398',\n",
       "  'pixel399',\n",
       "  'pixel400',\n",
       "  'pixel401',\n",
       "  'pixel402',\n",
       "  'pixel403',\n",
       "  'pixel404',\n",
       "  'pixel405',\n",
       "  'pixel406',\n",
       "  'pixel407',\n",
       "  'pixel408',\n",
       "  'pixel409',\n",
       "  'pixel410',\n",
       "  'pixel411',\n",
       "  'pixel412',\n",
       "  'pixel413',\n",
       "  'pixel414',\n",
       "  'pixel415',\n",
       "  'pixel416',\n",
       "  'pixel417',\n",
       "  'pixel418',\n",
       "  'pixel419',\n",
       "  'pixel420',\n",
       "  'pixel421',\n",
       "  'pixel422',\n",
       "  'pixel423',\n",
       "  'pixel424',\n",
       "  'pixel425',\n",
       "  'pixel426',\n",
       "  'pixel427',\n",
       "  'pixel428',\n",
       "  'pixel429',\n",
       "  'pixel430',\n",
       "  'pixel431',\n",
       "  'pixel432',\n",
       "  'pixel433',\n",
       "  'pixel434',\n",
       "  'pixel435',\n",
       "  'pixel436',\n",
       "  'pixel437',\n",
       "  'pixel438',\n",
       "  'pixel439',\n",
       "  'pixel440',\n",
       "  'pixel441',\n",
       "  'pixel442',\n",
       "  'pixel443',\n",
       "  'pixel444',\n",
       "  'pixel445',\n",
       "  'pixel446',\n",
       "  'pixel447',\n",
       "  'pixel448',\n",
       "  'pixel449',\n",
       "  'pixel450',\n",
       "  'pixel451',\n",
       "  'pixel452',\n",
       "  'pixel453',\n",
       "  'pixel454',\n",
       "  'pixel455',\n",
       "  'pixel456',\n",
       "  'pixel457',\n",
       "  'pixel458',\n",
       "  'pixel459',\n",
       "  'pixel460',\n",
       "  'pixel461',\n",
       "  'pixel462',\n",
       "  'pixel463',\n",
       "  'pixel464',\n",
       "  'pixel465',\n",
       "  'pixel466',\n",
       "  'pixel467',\n",
       "  'pixel468',\n",
       "  'pixel469',\n",
       "  'pixel470',\n",
       "  'pixel471',\n",
       "  'pixel472',\n",
       "  'pixel473',\n",
       "  'pixel474',\n",
       "  'pixel475',\n",
       "  'pixel476',\n",
       "  'pixel477',\n",
       "  'pixel478',\n",
       "  'pixel479',\n",
       "  'pixel480',\n",
       "  'pixel481',\n",
       "  'pixel482',\n",
       "  'pixel483',\n",
       "  'pixel484',\n",
       "  'pixel485',\n",
       "  'pixel486',\n",
       "  'pixel487',\n",
       "  'pixel488',\n",
       "  'pixel489',\n",
       "  'pixel490',\n",
       "  'pixel491',\n",
       "  'pixel492',\n",
       "  'pixel493',\n",
       "  'pixel494',\n",
       "  'pixel495',\n",
       "  'pixel496',\n",
       "  'pixel497',\n",
       "  'pixel498',\n",
       "  'pixel499',\n",
       "  'pixel500',\n",
       "  'pixel501',\n",
       "  'pixel502',\n",
       "  'pixel503',\n",
       "  'pixel504',\n",
       "  'pixel505',\n",
       "  'pixel506',\n",
       "  'pixel507',\n",
       "  'pixel508',\n",
       "  'pixel509',\n",
       "  'pixel510',\n",
       "  'pixel511',\n",
       "  'pixel512',\n",
       "  'pixel513',\n",
       "  'pixel514',\n",
       "  'pixel515',\n",
       "  'pixel516',\n",
       "  'pixel517',\n",
       "  'pixel518',\n",
       "  'pixel519',\n",
       "  'pixel520',\n",
       "  'pixel521',\n",
       "  'pixel522',\n",
       "  'pixel523',\n",
       "  'pixel524',\n",
       "  'pixel525',\n",
       "  'pixel526',\n",
       "  'pixel527',\n",
       "  'pixel528',\n",
       "  'pixel529',\n",
       "  'pixel530',\n",
       "  'pixel531',\n",
       "  'pixel532',\n",
       "  'pixel533',\n",
       "  'pixel534',\n",
       "  'pixel535',\n",
       "  'pixel536',\n",
       "  'pixel537',\n",
       "  'pixel538',\n",
       "  'pixel539',\n",
       "  'pixel540',\n",
       "  'pixel541',\n",
       "  'pixel542',\n",
       "  'pixel543',\n",
       "  'pixel544',\n",
       "  'pixel545',\n",
       "  'pixel546',\n",
       "  'pixel547',\n",
       "  'pixel548',\n",
       "  'pixel549',\n",
       "  'pixel550',\n",
       "  'pixel551',\n",
       "  'pixel552',\n",
       "  'pixel553',\n",
       "  'pixel554',\n",
       "  'pixel555',\n",
       "  'pixel556',\n",
       "  'pixel557',\n",
       "  'pixel558',\n",
       "  'pixel559',\n",
       "  'pixel560',\n",
       "  'pixel561',\n",
       "  'pixel562',\n",
       "  'pixel563',\n",
       "  'pixel564',\n",
       "  'pixel565',\n",
       "  'pixel566',\n",
       "  'pixel567',\n",
       "  'pixel568',\n",
       "  'pixel569',\n",
       "  'pixel570',\n",
       "  'pixel571',\n",
       "  'pixel572',\n",
       "  'pixel573',\n",
       "  'pixel574',\n",
       "  'pixel575',\n",
       "  'pixel576',\n",
       "  'pixel577',\n",
       "  'pixel578',\n",
       "  'pixel579',\n",
       "  'pixel580',\n",
       "  'pixel581',\n",
       "  'pixel582',\n",
       "  'pixel583',\n",
       "  'pixel584',\n",
       "  'pixel585',\n",
       "  'pixel586',\n",
       "  'pixel587',\n",
       "  'pixel588',\n",
       "  'pixel589',\n",
       "  'pixel590',\n",
       "  'pixel591',\n",
       "  'pixel592',\n",
       "  'pixel593',\n",
       "  'pixel594',\n",
       "  'pixel595',\n",
       "  'pixel596',\n",
       "  'pixel597',\n",
       "  'pixel598',\n",
       "  'pixel599',\n",
       "  'pixel600',\n",
       "  'pixel601',\n",
       "  'pixel602',\n",
       "  'pixel603',\n",
       "  'pixel604',\n",
       "  'pixel605',\n",
       "  'pixel606',\n",
       "  'pixel607',\n",
       "  'pixel608',\n",
       "  'pixel609',\n",
       "  'pixel610',\n",
       "  'pixel611',\n",
       "  'pixel612',\n",
       "  'pixel613',\n",
       "  'pixel614',\n",
       "  'pixel615',\n",
       "  'pixel616',\n",
       "  'pixel617',\n",
       "  'pixel618',\n",
       "  'pixel619',\n",
       "  'pixel620',\n",
       "  'pixel621',\n",
       "  'pixel622',\n",
       "  'pixel623',\n",
       "  'pixel624',\n",
       "  'pixel625',\n",
       "  'pixel626',\n",
       "  'pixel627',\n",
       "  'pixel628',\n",
       "  'pixel629',\n",
       "  'pixel630',\n",
       "  'pixel631',\n",
       "  'pixel632',\n",
       "  'pixel633',\n",
       "  'pixel634',\n",
       "  'pixel635',\n",
       "  'pixel636',\n",
       "  'pixel637',\n",
       "  'pixel638',\n",
       "  'pixel639',\n",
       "  'pixel640',\n",
       "  'pixel641',\n",
       "  'pixel642',\n",
       "  'pixel643',\n",
       "  'pixel644',\n",
       "  'pixel645',\n",
       "  'pixel646',\n",
       "  'pixel647',\n",
       "  'pixel648',\n",
       "  'pixel649',\n",
       "  'pixel650',\n",
       "  'pixel651',\n",
       "  'pixel652',\n",
       "  'pixel653',\n",
       "  'pixel654',\n",
       "  'pixel655',\n",
       "  'pixel656',\n",
       "  'pixel657',\n",
       "  'pixel658',\n",
       "  'pixel659',\n",
       "  'pixel660',\n",
       "  'pixel661',\n",
       "  'pixel662',\n",
       "  'pixel663',\n",
       "  'pixel664',\n",
       "  'pixel665',\n",
       "  'pixel666',\n",
       "  'pixel667',\n",
       "  'pixel668',\n",
       "  'pixel669',\n",
       "  'pixel670',\n",
       "  'pixel671',\n",
       "  'pixel672',\n",
       "  'pixel673',\n",
       "  'pixel674',\n",
       "  'pixel675',\n",
       "  'pixel676',\n",
       "  'pixel677',\n",
       "  'pixel678',\n",
       "  'pixel679',\n",
       "  'pixel680',\n",
       "  'pixel681',\n",
       "  'pixel682',\n",
       "  'pixel683',\n",
       "  'pixel684',\n",
       "  'pixel685',\n",
       "  'pixel686',\n",
       "  'pixel687',\n",
       "  'pixel688',\n",
       "  'pixel689',\n",
       "  'pixel690',\n",
       "  'pixel691',\n",
       "  'pixel692',\n",
       "  'pixel693',\n",
       "  'pixel694',\n",
       "  'pixel695',\n",
       "  'pixel696',\n",
       "  'pixel697',\n",
       "  'pixel698',\n",
       "  'pixel699',\n",
       "  'pixel700',\n",
       "  'pixel701',\n",
       "  'pixel702',\n",
       "  'pixel703',\n",
       "  'pixel704',\n",
       "  'pixel705',\n",
       "  'pixel706',\n",
       "  'pixel707',\n",
       "  'pixel708',\n",
       "  'pixel709',\n",
       "  'pixel710',\n",
       "  'pixel711',\n",
       "  'pixel712',\n",
       "  'pixel713',\n",
       "  'pixel714',\n",
       "  'pixel715',\n",
       "  'pixel716',\n",
       "  'pixel717',\n",
       "  'pixel718',\n",
       "  'pixel719',\n",
       "  'pixel720',\n",
       "  'pixel721',\n",
       "  'pixel722',\n",
       "  'pixel723',\n",
       "  'pixel724',\n",
       "  'pixel725',\n",
       "  'pixel726',\n",
       "  'pixel727',\n",
       "  'pixel728',\n",
       "  'pixel729',\n",
       "  'pixel730',\n",
       "  'pixel731',\n",
       "  'pixel732',\n",
       "  'pixel733',\n",
       "  'pixel734',\n",
       "  'pixel735',\n",
       "  'pixel736',\n",
       "  'pixel737',\n",
       "  'pixel738',\n",
       "  'pixel739',\n",
       "  'pixel740',\n",
       "  'pixel741',\n",
       "  'pixel742',\n",
       "  'pixel743',\n",
       "  'pixel744',\n",
       "  'pixel745',\n",
       "  'pixel746',\n",
       "  'pixel747',\n",
       "  'pixel748',\n",
       "  'pixel749',\n",
       "  'pixel750',\n",
       "  'pixel751',\n",
       "  'pixel752',\n",
       "  'pixel753',\n",
       "  'pixel754',\n",
       "  'pixel755',\n",
       "  'pixel756',\n",
       "  'pixel757',\n",
       "  'pixel758',\n",
       "  'pixel759',\n",
       "  'pixel760',\n",
       "  'pixel761',\n",
       "  'pixel762',\n",
       "  'pixel763',\n",
       "  'pixel764',\n",
       "  'pixel765',\n",
       "  'pixel766',\n",
       "  'pixel767',\n",
       "  'pixel768',\n",
       "  'pixel769',\n",
       "  'pixel770',\n",
       "  'pixel771',\n",
       "  'pixel772',\n",
       "  'pixel773',\n",
       "  'pixel774',\n",
       "  'pixel775',\n",
       "  'pixel776',\n",
       "  'pixel777',\n",
       "  'pixel778',\n",
       "  'pixel779',\n",
       "  'pixel780',\n",
       "  'pixel781',\n",
       "  'pixel782',\n",
       "  'pixel783',\n",
       "  'pixel784'],\n",
       " 'target_names': ['class'],\n",
       " 'DESCR': \"**Author**: Yann LeCun, Corinna Cortes, Christopher J.C. Burges  \\n**Source**: [MNIST Website](http://yann.lecun.com/exdb/mnist/) - Date unknown  \\n**Please cite**:  \\n\\nThe MNIST database of handwritten digits with 784 features, raw data available at: http://yann.lecun.com/exdb/mnist/. It can be split in a training set of the first 60,000 examples, and a test set of 10,000 examples  \\n\\nIt is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image. It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting. The original black and white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of the anti-aliasing technique used by the normalization algorithm. the images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field.  \\n\\nWith some classification methods (particularly template-based methods, such as SVM and K-nearest neighbors), the error rate improves when the digits are centered by bounding box rather than center of mass. If you do this kind of pre-processing, you should report it in your publications. The MNIST database was constructed from NIST's NIST originally designated SD-3 as their training set and SD-1 as their test set. However, SD-3 is much cleaner and easier to recognize than SD-1. The reason for this can be found on the fact that SD-3 was collected among Census Bureau employees, while SD-1 was collected among high-school students. Drawing sensible conclusions from learning experiments requires that the result be independent of the choice of training set and test among the complete set of samples. Therefore it was necessary to build a new database by mixing NIST's datasets.  \\n\\nThe MNIST training set is composed of 30,000 patterns from SD-3 and 30,000 patterns from SD-1. Our test set was composed of 5,000 patterns from SD-3 and 5,000 patterns from SD-1. The 60,000 pattern training set contained examples from approximately 250 writers. We made sure that the sets of writers of the training set and test set were disjoint. SD-1 contains 58,527 digit images written by 500 different writers. In contrast to SD-3, where blocks of data from each writer appeared in sequence, the data in SD-1 is scrambled. Writer identities for SD-1 is available and we used this information to unscramble the writers. We then split SD-1 in two: characters written by the first 250 writers went into our new training set. The remaining 250 writers were placed in our test set. Thus we had two sets with nearly 30,000 examples each. The new training set was completed with enough examples from SD-3, starting at pattern # 0, to make a full set of 60,000 training patterns. Similarly, the new test set was completed with SD-3 examples starting at pattern # 35,000 to make a full set with 60,000 test patterns. Only a subset of 10,000 test images (5,000 from SD-1 and 5,000 from SD-3) is available on this site. The full 60,000 sample training set is available.\\n\\nDownloaded from openml.org.\",\n",
       " 'details': {'id': '554',\n",
       "  'name': 'mnist_784',\n",
       "  'version': '1',\n",
       "  'description_version': '1',\n",
       "  'format': 'ARFF',\n",
       "  'creator': ['Yann LeCun', 'Corinna Cortes', 'Christopher J.C. Burges'],\n",
       "  'upload_date': '2014-09-29T03:28:38',\n",
       "  'language': 'English',\n",
       "  'licence': 'Public',\n",
       "  'url': 'https://api.openml.org/data/v1/download/52667/mnist_784.arff',\n",
       "  'parquet_url': 'http://openml1.win.tue.nl/dataset554/dataset_554.pq',\n",
       "  'file_id': '52667',\n",
       "  'default_target_attribute': 'class',\n",
       "  'tag': ['AzurePilot',\n",
       "   'OpenML-CC18',\n",
       "   'OpenML100',\n",
       "   'study_1',\n",
       "   'study_123',\n",
       "   'study_41',\n",
       "   'study_99',\n",
       "   'vision'],\n",
       "  'visibility': 'public',\n",
       "  'minio_url': 'http://openml1.win.tue.nl/dataset554/dataset_554.pq',\n",
       "  'status': 'active',\n",
       "  'processing_date': '2020-11-20 20:12:09',\n",
       "  'md5_checksum': '0298d579eb1b86163de7723944c7e495'},\n",
       " 'url': 'https://www.openml.org/d/554'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_28 #visualize the data stored in the variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "rdg9fXJLvBJL"
   },
   "outputs": [],
   "source": [
    "mnist_28_img= mnist_28.data.to_numpy() #Convert the 'data' key in the dataset into a numpy array\n",
    "#The mnist_28.data consists of 784-pixel values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bX8G-CwQvHcE",
    "outputId": "aa970aa2-9894-4b91-cea7-6423bcafa4ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_28_img.shape #The shape of the numpy array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "smYgTNmovS5f"
   },
   "outputs": [],
   "source": [
    "#let us display one image from the data set\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "Hw6NxBBJvajw",
    "outputId": "7d66446b-e738-494a-b5e9-42adde067bf5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1de3003f150>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow((mnist_28_img[1]).reshape(28,28), cmap=plt.cm.gray) #inorder to display the image, we have to convert the 1-D array into a\n",
    "# a matrix of 28 by 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwYVxaE6K0_K"
   },
   "source": [
    "### 1.2 Tasks  \n",
    "\n",
    "1) Data Pre-Processing\n",
    "\n",
    " - Reshaping each row in (28, 28) shaped array\n",
    " - Splitting the data set into training and testing sets\n",
    " - One-hot Encoding for the classification\n",
    "\n",
    "2) Defining Function classes for CNN\n",
    "\n",
    " - Linear Layer\n",
    " - ReLU Layer\n",
    " - Softmax activation With Cross Entropy Loss\n",
    " - 2D - Convolution\n",
    " - 2D Max Pooling\n",
    " - Stochastic Gradient Descent Optimization\n",
    "\n",
    "3) Training and Testing Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Data Preprocessing - Split and Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "CFj2CVqVRWFE"
   },
   "outputs": [],
   "source": [
    "# Split the dataset and reshape the images\n",
    "X_train = mnist_28_img[0:60000][:].reshape(-1, 28, 28, 1) #The train data set\n",
    "X_test = mnist_28_img[60000:][:].reshape(-1, 28, 28, 1) #The test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IUcSOVrEwwU5",
    "outputId": "3ed02648-cd42-4758-bc96-3c2ab38478f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 28, 28, 1)\n",
      "X_test shape: (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "#Print the shapes of the X-values of the train and test data\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "HL8j5ujX-1sn"
   },
   "outputs": [],
   "source": [
    "# Convert labels to numpy array and then to integers\n",
    "mnist_28_labels = mnist_28.target.to_numpy().astype(int)\n",
    "\n",
    "# Split the labels\n",
    "y_train = mnist_28_labels[0:60000]\n",
    "y_test = mnist_28_labels[60000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0cRg-SQY-9M3",
    "outputId": "7904d11d-dbbb-492b-b019-118a4aefa730"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape: (60000,)\n",
      "y_test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Print the shapes of the y-values or the target variable of the train and test data sets.\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WV72j79Jwq6C"
   },
   "source": [
    "### 1.4 One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "U2fY0g-mwqZx"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "One-hot encoding of labels helps represent categorical variables as binary vectors. \n",
    "This enables the machine-learning algorithms to function directly without any bias or incompatibility.\n",
    "Here, we convert each of the digits from 0 to 9 into an array of zeros and ones such that the \n",
    "position in the array where it is equal to the value will be 1 and the rest will be zero.\n",
    "'''\n",
    "def one_hot_encode(labels, num_classes=10):\n",
    "    \"\"\" One-hot encode the labels.\"\"\"\n",
    "    encoded = np.zeros((labels.shape[0], num_classes))\n",
    "    for idx, label in enumerate(labels):\n",
    "        encoded[idx, label] = 1\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kbWYYnjBxIaR",
    "outputId": "5a7c5d72-1c9d-4a46-c2f1-dc0ba795a5a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape: (60000, 10)\n",
      "y_test shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Implementing the one-hot encoding to target variable\n",
    "y_train = one_hot_encode(y_train)\n",
    "y_test = one_hot_encode(y_test)\n",
    "\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijlbAGXkD9Gg"
   },
   "source": [
    "# 2. Defining Function classes for CNN\n",
    "\n",
    "### 2.1 Linear Layer (Fully Connected)\n",
    "<p>This layer will contain weights and biases. During the forward pass, we'll compute the matrix multiplication and addition:</p>\n",
    "<code>X.W + b</code>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "mjgiGfDLxmQY"
   },
   "outputs": [],
   "source": [
    "#Defining the CNN components\n",
    "\n",
    "#Linear Layer (Fully Connected Layer)\n",
    "\n",
    "'''\n",
    "Class variables\n",
    "weights: These are small random values that break symmetry and speed up convergence\n",
    "biases: Initially set to zero, on further movement it is updated to balance.\n",
    "gradients: These are gradients with respect to weights (dw) and biases (db) initialized to None.\n",
    "input variable (z): It is used to store the input for backpropagation.\n",
    "Class functions\n",
    "forward: enables forward propagation of linear layer by returning (X.W + b)\n",
    "backward: enables backpropagation of the linear layer by returning (grad.(W.T))\n",
    "'''\n",
    "class Linear:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.weights = np.random.randn(input_dim, output_dim) * 0.01\n",
    "        self.biases = np.zeros((1, output_dim))\n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "        self.z = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.z = x\n",
    "        return np.dot(x, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        #Gradient of weights: dot product of input transposed and incoming gradient\n",
    "        self.dw = np.dot(self.z.T, gradient)\n",
    "        #Gradient of biases: sum the incoming gradient along axis 0 (rows)\n",
    "        self.db = np.sum(gradient, axis=0, keepdims=True)\n",
    "        #Compute gradient with respect to the input to pass to the previous layer\n",
    "        return np.dot(gradient, self.weights.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCSfKZLjEBvw"
   },
   "source": [
    "### 2.2 ReLU Layer\n",
    "<p>The ReLU (Rectified Linear Unit) activation function will transform input values <code>x</code> to:</p>\n",
    "<code>max(0, x)</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "wWxV5aQiDDkI"
   },
   "outputs": [],
   "source": [
    "# ReLU Layer\n",
    "'''\n",
    "Class variables\n",
    "input: initializes the input variable to be none\n",
    "Class functions\n",
    "forward: Forward propagation method for ReLU activation function returning either zero or the input, whichever is the maximum\n",
    "backward: Backward propagation method for ReLU activation function returning gradient * (self.input > 0)\n",
    "'''\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Stores the input for later use in backpropagation\n",
    "        self.input = x\n",
    "        #Computes the element-wise maximum of 0 and input x, returning the result\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        # Computes the gradient of the ReLU activation function\n",
    "        # Returns gradient * 1 if input > 0, else returns gradient * 0\n",
    "        #Element-wise multiplication of incoming gradient with the ReLU gradient which is boolean array for input > 0\n",
    "        return gradient * (self.input > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZE30CqqnEFkg"
   },
   "source": [
    "### 2.3 Softmax Activation with Cross Entropy Loss\n",
    "<p>After the final layer, we'll use a softmax function to get probability distributions for multi-class classification. The cross-entropy loss will then measure the difference between the predicted and true distributions. The formula for the softmax function for a given input <code>z</code> is:</p>\n",
    "<code>softmax(z<sub>i</sub>) = exp(z<sub>i</sub>) / &Sigma; exp(z<sub>j</sub>)</code>\n",
    "<p>Where the summation is over all classes. The cross-entropy loss for true labels <code>y</code> and predicted probabilities <code>p</code> is:</p>\n",
    "<code>-&Sigma; y<sub>i</sub> log(p<sub>i</sub>)</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "R8cP4leBDFcx"
   },
   "outputs": [],
   "source": [
    "# Softmax Activation with Cross Entropy Loss for multi-class classifications\n",
    "'''\n",
    "Class functions\n",
    "forward: computes the Softmax function for the input logits, \n",
    "        calculates probabilities by normalizing the exponentials, \n",
    "        computes the negative log-likelihood loss of the true labels under the predicted probabilities, \n",
    "        and returns the loss normalized by the batch size.\n",
    "backward: computes the gradient of the Softmax Cross-Entropy loss with respect to the predicted probabilities. \n",
    "        It returns the difference between predicted probabilities and true labels, normalized by the batch size, \n",
    "        which is used during backpropagation to update the model's parameters.\n",
    "'''\n",
    "class SoftmaxCrossEntropy:\n",
    "    def __init__(self):\n",
    "        #Initializes variables to store computed probabilities and true labels\n",
    "        self.probabilities = None\n",
    "        self.true_labels = None\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        #Compute exponentials of input logits after adjusting for numerical stability\n",
    "        exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "        #Compute probabilities by normalizing the exponentials\n",
    "        self.probabilities = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "        #Store true labels for later use in backpropagation\n",
    "        self.true_labels = labels\n",
    "        #Compute and return the loss: negative log-likelihood of the true labels under the predicted probabilities\n",
    "        return -np.sum(labels * np.log(self.probabilities + 1e-15)) / logits.shape[0]\n",
    "\n",
    "    def backward(self):\n",
    "        #Compute the gradient of the loss with respect to the predicted probabilities\n",
    "        #The gradient is the difference between predicted probabilities and true labels, normalized by the batch size\n",
    "        return (self.probabilities - self.true_labels) / self.true_labels.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSdxL1pVEKoA"
   },
   "source": [
    "### 2.4 2D - Convolution\n",
    "<p>The convolutional layer will contain filters (or kernels) that will be convolved with the input image to detect patterns. The convolution operation for an input <code>I</code> and a filter <code>F</code> is:</p>\n",
    "<code>(I &ast; F)(x, y) = &Sigma; &Sigma; I(x - i, y - j) F(i, j)</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "3rJ9c1N7DHdY"
   },
   "outputs": [],
   "source": [
    "# 2D - Convolution with backward pass (taking channels into account)\n",
    "'''\n",
    "Class functions\n",
    "forward: performs the forward propagation for the convolutional layer, \n",
    "        computing the convolution operation between the input and the filters.\n",
    "backward: performs backpropagation for the convolutional layer, \n",
    "        computing gradients for both filters and the input tensor. \n",
    "        The filters are updated using the computed gradients with a learning rate of 0.01. \n",
    "        The method returns the computed gradients for the input tensor, \n",
    "        which are used for further backpropagation through the network\n",
    "'''\n",
    "class Conv2D:\n",
    "    def __init__(self, num_filters, filter_size, input_channels=1):\n",
    "        #Initializes the convolutional layer with specified number of filters, filter size, and input channels\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_size = filter_size\n",
    "        #Initialize filters with small random values, normalized by 9 for stability\n",
    "        self.filters = np.random.randn(num_filters, filter_size, filter_size, input_channels) / 9\n",
    "        #Variable to store the input for later use in backpropagation\n",
    "        self.input = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        #Store the input for later use in backpropagation\n",
    "        self.input = input\n",
    "        batch_size, h, w, _ = input.shape\n",
    "        #Initialize output tensor with appropriate dimensions\n",
    "        output = np.zeros((batch_size, h - self.filter_size + 1, w - self.filter_size + 1, self.num_filters))\n",
    "\n",
    "        #Perform convolution operation using the initialized filters and input\n",
    "        for b in range(batch_size):\n",
    "            for i in range(output.shape[1]):\n",
    "                for j in range(output.shape[2]):\n",
    "                    for f in range(self.num_filters):\n",
    "                        #Compute the convolution operation and populate the output tensor\n",
    "                        output[b, i, j, f] = np.sum(input[b, i:i+self.filter_size, j:j+self.filter_size] * self.filters[f])\n",
    "        #Return the output tensor after convolution\n",
    "        return output\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        batch_size, h, w, num_filters = gradient.shape\n",
    "        #Initialize gradients for filters and input tensor\n",
    "        d_filters = np.zeros_like(self.filters)\n",
    "        d_input = np.zeros_like(self.input)\n",
    "\n",
    "        #Compute gradients for filters and input tensor during backpropagation\n",
    "        for b in range(batch_size):\n",
    "            for i in range(h):\n",
    "                for j in range(w):\n",
    "                    for f in range(num_filters):\n",
    "                        #Update gradients for filters using the chain rule\n",
    "                        d_filters[f] += gradient[b, i, j, f] * self.input[b, i:i+self.filter_size, j:j+self.filter_size]\n",
    "                        #Update gradients for input tensor using the chain rule\n",
    "                        d_input[b, i:i+self.filter_size, j:j+self.filter_size] += gradient[b, i, j, f] * self.filters[f]\n",
    "\n",
    "        # Update the filters using the computed gradients with a learning rate of 0.01\n",
    "        self.filters -= 0.01 * d_filters\n",
    "\n",
    "        #Return the computed gradients for the input tensor, which will be used for further backpropagation\n",
    "        return d_input\n",
    "\n",
    "# This class now assumes a single channel (grayscale) for the input by default.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FO6WiZQZEN3A"
   },
   "source": [
    "### 2.5 2D Max Pooling\n",
    "<p>Max pooling will downsample the input based on the maximum value in a region. For a given region <code>R</code> in the input:</p>\n",
    "<code>max_pool(R) = max(R)</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "5YIE7BqwDMrH"
   },
   "outputs": [],
   "source": [
    "# 2D Max Pooling with backward pass\n",
    "'''\n",
    "Class functions\n",
    "forward: performs max pooling on the input, reducing the dimensions by selecting the \n",
    "        maximum value within non-overlapping regions.\n",
    "backward: computes gradients for the input tensor during backpropagation. \n",
    "        It uses the max values computed during forward propagation and assigns the incoming gradient to the \n",
    "        location of the maximum value, propagating it back to the corresponding input region.\n",
    "'''\n",
    "class MaxPooling2D:\n",
    "    def __init__(self, pool_size=2):\n",
    "        #Initializes the max pooling layer with a specified pool size\n",
    "        self.pool_size = pool_size\n",
    "        #Variable to store the input for later use in backpropagation\n",
    "        self.input = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        #Store the input for later use in backpropagation\n",
    "        self.input = input\n",
    "        batch_size, h, w, num_filters = input.shape\n",
    "        #Initialize output tensor with reduced dimensions after pooling\n",
    "        output = np.zeros((batch_size, h // self.pool_size, w // self.pool_size, num_filters))\n",
    "\n",
    "        #Perform max pooling operation on input\n",
    "        for b in range(batch_size):\n",
    "            for i in range(0, h, self.pool_size):\n",
    "                for j in range(0, w, self.pool_size):\n",
    "                    #Extract the region of interest and find the maximum value in the region\n",
    "                    output[b, i//self.pool_size, j//self.pool_size] = np.amax(input[b, i:i+self.pool_size, j:j+self.pool_size], axis=(0, 1))\n",
    "        #Return the output tensor after max pooling\n",
    "        return output\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        batch_size, h, w, num_filters = gradient.shape\n",
    "        #Initialize gradients for the input tensor\n",
    "        d_input = np.zeros_like(self.input)\n",
    "\n",
    "        #Compute gradients for input tensor during backpropagation\n",
    "        for b in range(batch_size):\n",
    "            for i in range(0, h * self.pool_size, self.pool_size):\n",
    "                for j in range(0, w * self.pool_size, self.pool_size):\n",
    "                    for f in range(num_filters):\n",
    "                        #Extract the region of interest\n",
    "                        region = self.input[b, i:i+self.pool_size, j:j+self.pool_size, f]\n",
    "                        max_val = np.amax(region)\n",
    "                        #Create a mask of the region with the max value\n",
    "                        mask = (region == max_val)\n",
    "                        #Update gradients for input tensor using the mask and incoming gradient\n",
    "                        d_input[b, i:i+self.pool_size, j:j+self.pool_size, f] += mask * gradient[b, i//self.pool_size, j//self.pool_size, f]\n",
    "        #Return the computed gradients for the input tensor, which will be used for further backpropagation\n",
    "        return d_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5F1qRq7EP6Z"
   },
   "source": [
    "\n",
    "### 2.6 Stochastic Gradient Descent (SGD) Optimization\n",
    "<p>SGD will be used to update the weights based on the gradients computed during backpropagation. The weight update rule is:</p>\n",
    "<code>W<sub>new</sub> = W<sub>old</sub> - &alpha; &times; dW</code>\n",
    "<p>Where <code>&alpha;</code> is the learning rate and <code>dW</code> is the gradient of the loss with respect to the weights.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "NcQQrh9dDOzn"
   },
   "outputs": [],
   "source": [
    "#Stochastic Gradient Descent (SGD) Optimization\n",
    "'''\n",
    "Class function\n",
    "update: performs parameter updates for a given layer using the gradients (dw for weights and db for biases) \n",
    "        computed during backpropagation. It adjusts the weights and biases in the opposite direction of the gradients, \n",
    "        scaled by the learning rate, in order to minimize the loss function during training.\n",
    "'''\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        #Initializes the SGD optimizer with a learning rate (default is 0.01)\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, layer):\n",
    "        #Update weights and biases of the given layer using the computed gradients and learning rate\n",
    "        layer.weights -= self.lr * layer.dw\n",
    "        layer.biases -= self.lr * layer.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8o2eGAcME8BA"
   },
   "source": [
    "# 3. Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I30lGO7nFKdw"
   },
   "source": [
    "### 3.1 Creating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "At89xgv5DQ1O"
   },
   "outputs": [],
   "source": [
    "#Combining all the layers into a single CNN model class.\n",
    "'''\n",
    "Class functions\n",
    "forward: performs forward propagation through the model, including convolution, activation, pooling, flattening, and fully connected layer operations. \n",
    "        It computes and returns the loss using Softmax Cross-Entropy loss function.\n",
    "backward: performs backward propagation through the model. It starts by computing gradients using the Softmax Cross-Entropy loss function, \n",
    "        then backpropagates through the fully connected layer, reshapes the gradient, \n",
    "        and further backpropagates through the max pooling layer, ReLU activation, and convolutional layer. \n",
    "        The gradients are used to update the parameters of the model during training.\n",
    "'''\n",
    "class MnistCNN:\n",
    "    def __init__(self):\n",
    "        #Initializes the layers and components of the CNN model for MNIST digit classification\n",
    "        #Convolutional layer with 8 filters and a filter size of 3x3\n",
    "        self.conv = Conv2D(num_filters=8, filter_size=3)\n",
    "        #Max pooling layer with a pool size of 2x2\n",
    "        self.pool = MaxPooling2D(pool_size=2)\n",
    "        #Fully connected (dense) layer with 10 output units (for 10 classes)\n",
    "        self.fc = Linear(13*13*8, 10)\n",
    "        #ReLU activation function\n",
    "        self.relu = ReLU()\n",
    "        #Softmax Cross-Entropy loss activation function for classification\n",
    "        self.loss_activation = SoftmaxCrossEntropy()\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        #Perform convolution, ReLU activation, max pooling, and flattening\n",
    "        x = self.conv.forward(x)\n",
    "        x = self.relu.forward(x)\n",
    "        x = self.pool.forward(x)\n",
    "        x = x.reshape(x.shape[0], -1)  # Flatten\n",
    "        #Perform forward propagation through the fully connected layer\n",
    "        x = self.fc.forward(x)\n",
    "        #Compute the loss using Softmax Cross-Entropy activation function\n",
    "        loss = self.loss_activation.forward(x, labels)\n",
    "        #Return the computed loss\n",
    "        return loss\n",
    "\n",
    "    def backward(self):\n",
    "        #Compute gradients using Softmax Cross-Entropy loss function\n",
    "        gradient = self.loss_activation.backward()\n",
    "        #Backpropagate through the fully connected layer and reshape the gradient\n",
    "        gradient = self.fc.backward(gradient)\n",
    "        gradient = gradient.reshape(-1, 13, 13, 8)  # Reshape back to match pooling layer shape\n",
    "        #Backpropagate through the max pooling layer, ReLU activation, and convolutional layer\n",
    "        gradient = self.pool.backward(gradient)\n",
    "        gradient = self.relu.backward(gradient)\n",
    "        gradient = self.conv.backward(gradient)\n",
    "        #Not returning anything, as the gradients update the parameters during backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFGAdv-WFUOw"
   },
   "source": [
    "### 3.2 Training the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kzIZ47gyFU7P",
    "outputId": "2f50560d-ef11-432e-d64f-2c3e106767b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.13859506146674858\n",
      "Epoch 2, Loss: 0.12858069986739087\n",
      "Epoch 3, Loss: 0.1244833985020946\n",
      "Epoch 4, Loss: 0.12431455666813777\n",
      "Epoch 5, Loss: 0.12578034587651382\n"
     ]
    }
   ],
   "source": [
    "#Create an instance of the MnistCNN model\n",
    "model = MnistCNN()\n",
    "#Instantiate Stochastic Gradient Descent (SGD) optimizer with a learning rate of 0.01v\n",
    "optimizer = SGD(lr=0.01)\n",
    "num_epochs = 5 #Number of epochs (complete passes through the dataset)\n",
    "batch_size = 250 #Batch size for training data\n",
    "\n",
    "#Loop through each epoch for training\n",
    "for epoch in range(num_epochs):\n",
    "    #Iterate through the training data in batches\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        #Extract a batch of input samples and their corresponding labels\n",
    "        batch_X = X_train[i:i+batch_size]\n",
    "        batch_y = y_train[i:i+batch_size]\n",
    "\n",
    "        #Forward pass: compute the loss for the current batch\n",
    "        loss = model.forward(batch_X, batch_y)\n",
    "\n",
    "        #Backward pass: compute gradients and update model parameters\n",
    "        model.backward()\n",
    "\n",
    "        #Update the fully connected layer's weights using SGD\n",
    "        optimizer.update(model.fc)\n",
    "\n",
    "    #Print loss for the current epoch\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87rOTVHSFXew"
   },
   "source": [
    "### 3.3 Testing the model \n",
    "\n",
    "<b>By testing the model, we find the accuracy of the model in classifying the digits in the MNIST dataset</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "8Q2FNs5gnPSN",
    "outputId": "d362d84c-f5f4-404a-9ef4-d588793ab50f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 96.44%\n"
     ]
    }
   ],
   "source": [
    "#Initialize a variable to keep track of the number of correct predictions\n",
    "correct_predictions = 0\n",
    "#Iterate through the test dataset\n",
    "for i in range(len(X_test)):\n",
    "    # Reshape the test image to include a batch dimension (1, 28, 28, 1)\n",
    "    test_image = X_test[i].reshape(1, 28, 28, 1)\n",
    "\n",
    "    #Perform forward pass through the convolutional, ReLU, and max pooling layers\n",
    "    pooled_output = model.pool.forward(model.relu.forward(model.conv.forward(test_image)))\n",
    "\n",
    "    # Flatten the output of the pooling layer\n",
    "    flattened_output = pooled_output.reshape(pooled_output.shape[0], -1)\n",
    "\n",
    "    #Perform forward pass through the fully connected layer to get logits\n",
    "    logits = model.fc.forward(flattened_output)\n",
    "    #Compute the predicted class (index with the highest logit score)\n",
    "    prediction = np.argmax(logits)\n",
    "    #Get the true label from the one-hot encoded target vector\n",
    "    true_label = np.argmax(y_test[i])\n",
    "    #Check if the prediction matches the true label\n",
    "    if prediction == true_label:\n",
    "        correct_predictions += 1\n",
    "\n",
    "#Calculate accuracy by dividing the number of correct predictions by the total number of test samples\n",
    "accuracy = correct_predictions / len(X_test)\n",
    "#Print the test accuracy percentage\n",
    "print(f\"Test accuracy: {accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Af3UlMvDEoIU"
   },
   "source": [
    "# Conclusion\n",
    "- In this implementation, a Convolutional Neural Network (CNN) model for classifying handwritten digits from the MNIST dataset was developed and trained. \n",
    "- The model architecture consists of a convolutional layer followed by ReLU activation, max-pooling, and a fully connected layer with softmax activation for multiclass classification.\n",
    "- The model was trained using the Stochastic Gradient Descent (SGD) optimizer over 5 epochs with a batch size of 250.\n",
    "### Training and testing of the model\n",
    "- During training, the model learned to recognize intricate patterns and features in the input images, enabling it to make accurate predictions.\n",
    "- The training process involved iterative forward and backward passes, adjusting the model's parameters to minimize the softmax cross-entropy loss.\n",
    "- After the training process, the model demonstrated a high level of accuracy on the unseen test dataset, achieving an accuracy of 96.44%.\n",
    "\n",
    "<b>This high test accuracy indicates the effectiveness of the CNN architecture and the optimization techniques used.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
