{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DlLCK6UgrrpW"
   },
   "source": [
    "# Classify the 10 digits using MNIST data\n",
    "\n",
    "Implement a convolutional neural network using `NumPy`ONLY to classify the digits. \n",
    "\n",
    "## Data Description\n",
    "[MNIST](https://en.wikipedia.org/wiki/MNIST_database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Extraction and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ohha6cR7ufYF"
   },
   "outputs": [],
   "source": [
    "# load the mnist data\n",
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ChXaGZKCulwM",
    "outputId": "a0f4d5ca-0bdb-4186-e16c-8f308bbf874a"
   },
   "outputs": [],
   "source": [
    "mnist_28 = fetch_openml('mnist_784', parser = 'auto') #fetch the mnist_784 dataset using the fetch_openml API and store it in a variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ar7Bie1YeZqR",
    "outputId": "ba7fd8fb-9cdd-464e-b415-773d841a9ebf"
   },
   "outputs": [],
   "source": [
    "#mnist_28 #visualize the data stored in the variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "rdg9fXJLvBJL"
   },
   "outputs": [],
   "source": [
    "mnist_28_img= mnist_28.data.to_numpy() #Convert the 'data' key in the dataset into a numpy array\n",
    "#The mnist_28.data consists of 784-pixel values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bX8G-CwQvHcE",
    "outputId": "aa970aa2-9894-4b91-cea7-6423bcafa4ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_28_img.shape #The shape of the numpy array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "smYgTNmovS5f"
   },
   "outputs": [],
   "source": [
    "#let us display one image from the data set\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "Hw6NxBBJvajw",
    "outputId": "7d66446b-e738-494a-b5e9-42adde067bf5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1de3003f150>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow((mnist_28_img[1]).reshape(28,28), cmap=plt.cm.gray) #inorder to display the image, we have to convert the 1-D array into a\n",
    "# a matrix of 28 by 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwYVxaE6K0_K"
   },
   "source": [
    "### 1.2 Tasks  \n",
    "\n",
    "1) Data Pre-Processing\n",
    "\n",
    " - Reshaping each row in (28, 28) shaped array\n",
    " - Splitting the data set into training and testing sets\n",
    " - One-hot Encoding for the classification\n",
    "\n",
    "2) Defining Function classes for CNN\n",
    "\n",
    " - Linear Layer\n",
    " - ReLU Layer\n",
    " - Softmax activation With Cross Entropy Loss\n",
    " - 2D - Convolution\n",
    " - 2D Max Pooling\n",
    " - Stochastic Gradient Descent Optimization\n",
    "\n",
    "3) Training and Testing Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Data Preprocessing - Split and Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "CFj2CVqVRWFE"
   },
   "outputs": [],
   "source": [
    "# Split the dataset and reshape the images\n",
    "X_train = mnist_28_img[0:60000][:].reshape(-1, 28, 28, 1) #The train data set\n",
    "X_test = mnist_28_img[60000:][:].reshape(-1, 28, 28, 1) #The test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IUcSOVrEwwU5",
    "outputId": "3ed02648-cd42-4758-bc96-3c2ab38478f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 28, 28, 1)\n",
      "X_test shape: (10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "#Print the shapes of the X-values of the train and test data\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "HL8j5ujX-1sn"
   },
   "outputs": [],
   "source": [
    "# Convert labels to numpy array and then to integers\n",
    "mnist_28_labels = mnist_28.target.to_numpy().astype(int)\n",
    "\n",
    "# Split the labels\n",
    "y_train = mnist_28_labels[0:60000]\n",
    "y_test = mnist_28_labels[60000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0cRg-SQY-9M3",
    "outputId": "7904d11d-dbbb-492b-b019-118a4aefa730"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape: (60000,)\n",
      "y_test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Print the shapes of the y-values or the target variable of the train and test data sets.\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WV72j79Jwq6C"
   },
   "source": [
    "### 1.4 One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "U2fY0g-mwqZx"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "One-hot encoding of labels helps represent categorical variables as binary vectors. \n",
    "This enables the machine-learning algorithms to function directly without any bias or incompatibility.\n",
    "Here, we convert each of the digits from 0 to 9 into an array of zeros and ones such that the \n",
    "position in the array where it is equal to the value will be 1 and the rest will be zero.\n",
    "'''\n",
    "def one_hot_encode(labels, num_classes=10):\n",
    "    \"\"\" One-hot encode the labels.\"\"\"\n",
    "    encoded = np.zeros((labels.shape[0], num_classes))\n",
    "    for idx, label in enumerate(labels):\n",
    "        encoded[idx, label] = 1\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kbWYYnjBxIaR",
    "outputId": "5a7c5d72-1c9d-4a46-c2f1-dc0ba795a5a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape: (60000, 10)\n",
      "y_test shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Implementing the one-hot encoding to target variable\n",
    "y_train = one_hot_encode(y_train)\n",
    "y_test = one_hot_encode(y_test)\n",
    "\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijlbAGXkD9Gg"
   },
   "source": [
    "# 2. Defining Function classes for CNN\n",
    "\n",
    "### 2.1 Linear Layer (Fully Connected)\n",
    "<p>This layer will contain weights and biases. During the forward pass, we'll compute the matrix multiplication and addition:</p>\n",
    "<code>X.W + b</code>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "mjgiGfDLxmQY"
   },
   "outputs": [],
   "source": [
    "#Defining the CNN components\n",
    "\n",
    "#Linear Layer (Fully Connected Layer)\n",
    "\n",
    "'''\n",
    "Class variables\n",
    "weights: These are small random values that break symmetry and speed up convergence\n",
    "biases: Initially set to zero, on further movement it is updated to balance.\n",
    "gradients: These are gradients with respect to weights (dw) and biases (db) initialized to None.\n",
    "input variable (z): It is used to store the input for backpropagation.\n",
    "Class functions\n",
    "forward: enables forward propagation of linear layer by returning (X.W + b)\n",
    "backward: enables backpropagation of the linear layer by returning (grad.(W.T))\n",
    "'''\n",
    "class Linear:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        self.weights = np.random.randn(input_dim, output_dim) * 0.01\n",
    "        self.biases = np.zeros((1, output_dim))\n",
    "        self.dw = None\n",
    "        self.db = None\n",
    "        self.z = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.z = x\n",
    "        return np.dot(x, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        #Gradient of weights: dot product of input transposed and incoming gradient\n",
    "        self.dw = np.dot(self.z.T, gradient)\n",
    "        #Gradient of biases: sum the incoming gradient along axis 0 (rows)\n",
    "        self.db = np.sum(gradient, axis=0, keepdims=True)\n",
    "        #Compute gradient with respect to the input to pass to the previous layer\n",
    "        return np.dot(gradient, self.weights.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCSfKZLjEBvw"
   },
   "source": [
    "### 2.2 ReLU Layer\n",
    "<p>The ReLU (Rectified Linear Unit) activation function will transform input values <code>x</code> to:</p>\n",
    "<code>max(0, x)</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "wWxV5aQiDDkI"
   },
   "outputs": [],
   "source": [
    "# ReLU Layer\n",
    "'''\n",
    "Class variables\n",
    "input: initializes the input variable to be none\n",
    "Class functions\n",
    "forward: Forward propagation method for ReLU activation function returning either zero or the input, whichever is the maximum\n",
    "backward: Backward propagation method for ReLU activation function returning gradient * (self.input > 0)\n",
    "'''\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Stores the input for later use in backpropagation\n",
    "        self.input = x\n",
    "        #Computes the element-wise maximum of 0 and input x, returning the result\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        # Computes the gradient of the ReLU activation function\n",
    "        # Returns gradient * 1 if input > 0, else returns gradient * 0\n",
    "        #Element-wise multiplication of incoming gradient with the ReLU gradient which is boolean array for input > 0\n",
    "        return gradient * (self.input > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZE30CqqnEFkg"
   },
   "source": [
    "### 2.3 Softmax Activation with Cross Entropy Loss\n",
    "<p>After the final layer, we'll use a softmax function to get probability distributions for multi-class classification. The cross-entropy loss will then measure the difference between the predicted and true distributions. The formula for the softmax function for a given input <code>z</code> is:</p>\n",
    "<code>softmax(z<sub>i</sub>) = exp(z<sub>i</sub>) / &Sigma; exp(z<sub>j</sub>)</code>\n",
    "<p>Where the summation is over all classes. The cross-entropy loss for true labels <code>y</code> and predicted probabilities <code>p</code> is:</p>\n",
    "<code>-&Sigma; y<sub>i</sub> log(p<sub>i</sub>)</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "R8cP4leBDFcx"
   },
   "outputs": [],
   "source": [
    "# Softmax Activation with Cross Entropy Loss for multi-class classifications\n",
    "'''\n",
    "Class functions\n",
    "forward: computes the Softmax function for the input logits, \n",
    "        calculates probabilities by normalizing the exponentials, \n",
    "        computes the negative log-likelihood loss of the true labels under the predicted probabilities, \n",
    "        and returns the loss normalized by the batch size.\n",
    "backward: computes the gradient of the Softmax Cross-Entropy loss with respect to the predicted probabilities. \n",
    "        It returns the difference between predicted probabilities and true labels, normalized by the batch size, \n",
    "        which is used during backpropagation to update the model's parameters.\n",
    "'''\n",
    "class SoftmaxCrossEntropy:\n",
    "    def __init__(self):\n",
    "        #Initializes variables to store computed probabilities and true labels\n",
    "        self.probabilities = None\n",
    "        self.true_labels = None\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        #Compute exponentials of input logits after adjusting for numerical stability\n",
    "        exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "        #Compute probabilities by normalizing the exponentials\n",
    "        self.probabilities = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "        #Store true labels for later use in backpropagation\n",
    "        self.true_labels = labels\n",
    "        #Compute and return the loss: negative log-likelihood of the true labels under the predicted probabilities\n",
    "        return -np.sum(labels * np.log(self.probabilities + 1e-15)) / logits.shape[0]\n",
    "\n",
    "    def backward(self):\n",
    "        #Compute the gradient of the loss with respect to the predicted probabilities\n",
    "        #The gradient is the difference between predicted probabilities and true labels, normalized by the batch size\n",
    "        return (self.probabilities - self.true_labels) / self.true_labels.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSdxL1pVEKoA"
   },
   "source": [
    "### 2.4 2D - Convolution\n",
    "<p>The convolutional layer will contain filters (or kernels) that will be convolved with the input image to detect patterns. The convolution operation for an input <code>I</code> and a filter <code>F</code> is:</p>\n",
    "<code>(I &ast; F)(x, y) = &Sigma; &Sigma; I(x - i, y - j) F(i, j)</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "3rJ9c1N7DHdY"
   },
   "outputs": [],
   "source": [
    "# 2D - Convolution with backward pass (taking channels into account)\n",
    "'''\n",
    "Class functions\n",
    "forward: performs the forward propagation for the convolutional layer, \n",
    "        computing the convolution operation between the input and the filters.\n",
    "backward: performs backpropagation for the convolutional layer, \n",
    "        computing gradients for both filters and the input tensor. \n",
    "        The filters are updated using the computed gradients with a learning rate of 0.01. \n",
    "        The method returns the computed gradients for the input tensor, \n",
    "        which are used for further backpropagation through the network\n",
    "'''\n",
    "class Conv2D:\n",
    "    def __init__(self, num_filters, filter_size, input_channels=1):\n",
    "        #Initializes the convolutional layer with specified number of filters, filter size, and input channels\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_size = filter_size\n",
    "        #Initialize filters with small random values, normalized by 9 for stability\n",
    "        self.filters = np.random.randn(num_filters, filter_size, filter_size, input_channels) / 9\n",
    "        #Variable to store the input for later use in backpropagation\n",
    "        self.input = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        #Store the input for later use in backpropagation\n",
    "        self.input = input\n",
    "        batch_size, h, w, _ = input.shape\n",
    "        #Initialize output tensor with appropriate dimensions\n",
    "        output = np.zeros((batch_size, h - self.filter_size + 1, w - self.filter_size + 1, self.num_filters))\n",
    "\n",
    "        #Perform convolution operation using the initialized filters and input\n",
    "        for b in range(batch_size):\n",
    "            for i in range(output.shape[1]):\n",
    "                for j in range(output.shape[2]):\n",
    "                    for f in range(self.num_filters):\n",
    "                        #Compute the convolution operation and populate the output tensor\n",
    "                        output[b, i, j, f] = np.sum(input[b, i:i+self.filter_size, j:j+self.filter_size] * self.filters[f])\n",
    "        #Return the output tensor after convolution\n",
    "        return output\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        batch_size, h, w, num_filters = gradient.shape\n",
    "        #Initialize gradients for filters and input tensor\n",
    "        d_filters = np.zeros_like(self.filters)\n",
    "        d_input = np.zeros_like(self.input)\n",
    "\n",
    "        #Compute gradients for filters and input tensor during backpropagation\n",
    "        for b in range(batch_size):\n",
    "            for i in range(h):\n",
    "                for j in range(w):\n",
    "                    for f in range(num_filters):\n",
    "                        #Update gradients for filters using the chain rule\n",
    "                        d_filters[f] += gradient[b, i, j, f] * self.input[b, i:i+self.filter_size, j:j+self.filter_size]\n",
    "                        #Update gradients for input tensor using the chain rule\n",
    "                        d_input[b, i:i+self.filter_size, j:j+self.filter_size] += gradient[b, i, j, f] * self.filters[f]\n",
    "\n",
    "        # Update the filters using the computed gradients with a learning rate of 0.01\n",
    "        self.filters -= 0.01 * d_filters\n",
    "\n",
    "        #Return the computed gradients for the input tensor, which will be used for further backpropagation\n",
    "        return d_input\n",
    "\n",
    "# This class now assumes a single channel (grayscale) for the input by default.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FO6WiZQZEN3A"
   },
   "source": [
    "### 2.5 2D Max Pooling\n",
    "<p>Max pooling will downsample the input based on the maximum value in a region. For a given region <code>R</code> in the input:</p>\n",
    "<code>max_pool(R) = max(R)</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "5YIE7BqwDMrH"
   },
   "outputs": [],
   "source": [
    "# 2D Max Pooling with backward pass\n",
    "'''\n",
    "Class functions\n",
    "forward: performs max pooling on the input, reducing the dimensions by selecting the \n",
    "        maximum value within non-overlapping regions.\n",
    "backward: computes gradients for the input tensor during backpropagation. \n",
    "        It uses the max values computed during forward propagation and assigns the incoming gradient to the \n",
    "        location of the maximum value, propagating it back to the corresponding input region.\n",
    "'''\n",
    "class MaxPooling2D:\n",
    "    def __init__(self, pool_size=2):\n",
    "        #Initializes the max pooling layer with a specified pool size\n",
    "        self.pool_size = pool_size\n",
    "        #Variable to store the input for later use in backpropagation\n",
    "        self.input = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        #Store the input for later use in backpropagation\n",
    "        self.input = input\n",
    "        batch_size, h, w, num_filters = input.shape\n",
    "        #Initialize output tensor with reduced dimensions after pooling\n",
    "        output = np.zeros((batch_size, h // self.pool_size, w // self.pool_size, num_filters))\n",
    "\n",
    "        #Perform max pooling operation on input\n",
    "        for b in range(batch_size):\n",
    "            for i in range(0, h, self.pool_size):\n",
    "                for j in range(0, w, self.pool_size):\n",
    "                    #Extract the region of interest and find the maximum value in the region\n",
    "                    output[b, i//self.pool_size, j//self.pool_size] = np.amax(input[b, i:i+self.pool_size, j:j+self.pool_size], axis=(0, 1))\n",
    "        #Return the output tensor after max pooling\n",
    "        return output\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        batch_size, h, w, num_filters = gradient.shape\n",
    "        #Initialize gradients for the input tensor\n",
    "        d_input = np.zeros_like(self.input)\n",
    "\n",
    "        #Compute gradients for input tensor during backpropagation\n",
    "        for b in range(batch_size):\n",
    "            for i in range(0, h * self.pool_size, self.pool_size):\n",
    "                for j in range(0, w * self.pool_size, self.pool_size):\n",
    "                    for f in range(num_filters):\n",
    "                        #Extract the region of interest\n",
    "                        region = self.input[b, i:i+self.pool_size, j:j+self.pool_size, f]\n",
    "                        max_val = np.amax(region)\n",
    "                        #Create a mask of the region with the max value\n",
    "                        mask = (region == max_val)\n",
    "                        #Update gradients for input tensor using the mask and incoming gradient\n",
    "                        d_input[b, i:i+self.pool_size, j:j+self.pool_size, f] += mask * gradient[b, i//self.pool_size, j//self.pool_size, f]\n",
    "        #Return the computed gradients for the input tensor, which will be used for further backpropagation\n",
    "        return d_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5F1qRq7EP6Z"
   },
   "source": [
    "\n",
    "### 2.6 Stochastic Gradient Descent (SGD) Optimization\n",
    "<p>SGD will be used to update the weights based on the gradients computed during backpropagation. The weight update rule is:</p>\n",
    "<code>W<sub>new</sub> = W<sub>old</sub> - &alpha; &times; dW</code>\n",
    "<p>Where <code>&alpha;</code> is the learning rate and <code>dW</code> is the gradient of the loss with respect to the weights.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "NcQQrh9dDOzn"
   },
   "outputs": [],
   "source": [
    "#Stochastic Gradient Descent (SGD) Optimization\n",
    "'''\n",
    "Class function\n",
    "update: performs parameter updates for a given layer using the gradients (dw for weights and db for biases) \n",
    "        computed during backpropagation. It adjusts the weights and biases in the opposite direction of the gradients, \n",
    "        scaled by the learning rate, in order to minimize the loss function during training.\n",
    "'''\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        #Initializes the SGD optimizer with a learning rate (default is 0.01)\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, layer):\n",
    "        #Update weights and biases of the given layer using the computed gradients and learning rate\n",
    "        layer.weights -= self.lr * layer.dw\n",
    "        layer.biases -= self.lr * layer.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8o2eGAcME8BA"
   },
   "source": [
    "# 3. Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I30lGO7nFKdw"
   },
   "source": [
    "### 3.1 Creating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "At89xgv5DQ1O"
   },
   "outputs": [],
   "source": [
    "#Combining all the layers into a single CNN model class.\n",
    "'''\n",
    "Class functions\n",
    "forward: performs forward propagation through the model, including convolution, activation, pooling, flattening, and fully connected layer operations. \n",
    "        It computes and returns the loss using Softmax Cross-Entropy loss function.\n",
    "backward: performs backward propagation through the model. It starts by computing gradients using the Softmax Cross-Entropy loss function, \n",
    "        then backpropagates through the fully connected layer, reshapes the gradient, \n",
    "        and further backpropagates through the max pooling layer, ReLU activation, and convolutional layer. \n",
    "        The gradients are used to update the parameters of the model during training.\n",
    "'''\n",
    "class MnistCNN:\n",
    "    def __init__(self):\n",
    "        #Initializes the layers and components of the CNN model for MNIST digit classification\n",
    "        #Convolutional layer with 8 filters and a filter size of 3x3\n",
    "        self.conv = Conv2D(num_filters=8, filter_size=3)\n",
    "        #Max pooling layer with a pool size of 2x2\n",
    "        self.pool = MaxPooling2D(pool_size=2)\n",
    "        #Fully connected (dense) layer with 10 output units (for 10 classes)\n",
    "        self.fc = Linear(13*13*8, 10)\n",
    "        #ReLU activation function\n",
    "        self.relu = ReLU()\n",
    "        #Softmax Cross-Entropy loss activation function for classification\n",
    "        self.loss_activation = SoftmaxCrossEntropy()\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        #Perform convolution, ReLU activation, max pooling, and flattening\n",
    "        x = self.conv.forward(x)\n",
    "        x = self.relu.forward(x)\n",
    "        x = self.pool.forward(x)\n",
    "        x = x.reshape(x.shape[0], -1)  # Flatten\n",
    "        #Perform forward propagation through the fully connected layer\n",
    "        x = self.fc.forward(x)\n",
    "        #Compute the loss using Softmax Cross-Entropy activation function\n",
    "        loss = self.loss_activation.forward(x, labels)\n",
    "        #Return the computed loss\n",
    "        return loss\n",
    "\n",
    "    def backward(self):\n",
    "        #Compute gradients using Softmax Cross-Entropy loss function\n",
    "        gradient = self.loss_activation.backward()\n",
    "        #Backpropagate through the fully connected layer and reshape the gradient\n",
    "        gradient = self.fc.backward(gradient)\n",
    "        gradient = gradient.reshape(-1, 13, 13, 8)  # Reshape back to match pooling layer shape\n",
    "        #Backpropagate through the max pooling layer, ReLU activation, and convolutional layer\n",
    "        gradient = self.pool.backward(gradient)\n",
    "        gradient = self.relu.backward(gradient)\n",
    "        gradient = self.conv.backward(gradient)\n",
    "        #Not returning anything, as the gradients update the parameters during backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFGAdv-WFUOw"
   },
   "source": [
    "### 3.2 Training the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kzIZ47gyFU7P",
    "outputId": "2f50560d-ef11-432e-d64f-2c3e106767b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.13859506146674858\n",
      "Epoch 2, Loss: 0.12858069986739087\n",
      "Epoch 3, Loss: 0.1244833985020946\n",
      "Epoch 4, Loss: 0.12431455666813777\n",
      "Epoch 5, Loss: 0.12578034587651382\n"
     ]
    }
   ],
   "source": [
    "#Create an instance of the MnistCNN model\n",
    "model = MnistCNN()\n",
    "#Instantiate Stochastic Gradient Descent (SGD) optimizer with a learning rate of 0.01v\n",
    "optimizer = SGD(lr=0.01)\n",
    "num_epochs = 5 #Number of epochs (complete passes through the dataset)\n",
    "batch_size = 250 #Batch size for training data\n",
    "\n",
    "#Loop through each epoch for training\n",
    "for epoch in range(num_epochs):\n",
    "    #Iterate through the training data in batches\n",
    "    for i in range(0, len(X_train), batch_size):\n",
    "        #Extract a batch of input samples and their corresponding labels\n",
    "        batch_X = X_train[i:i+batch_size]\n",
    "        batch_y = y_train[i:i+batch_size]\n",
    "\n",
    "        #Forward pass: compute the loss for the current batch\n",
    "        loss = model.forward(batch_X, batch_y)\n",
    "\n",
    "        #Backward pass: compute gradients and update model parameters\n",
    "        model.backward()\n",
    "\n",
    "        #Update the fully connected layer's weights using SGD\n",
    "        optimizer.update(model.fc)\n",
    "\n",
    "    #Print loss for the current epoch\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "87rOTVHSFXew"
   },
   "source": [
    "### 3.3 Testing the model \n",
    "\n",
    "<b>By testing the model, we find the accuracy of the model in classifying the digits in the MNIST dataset</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "8Q2FNs5gnPSN",
    "outputId": "d362d84c-f5f4-404a-9ef4-d588793ab50f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 96.44%\n"
     ]
    }
   ],
   "source": [
    "#Initialize a variable to keep track of the number of correct predictions\n",
    "correct_predictions = 0\n",
    "#Iterate through the test dataset\n",
    "for i in range(len(X_test)):\n",
    "    # Reshape the test image to include a batch dimension (1, 28, 28, 1)\n",
    "    test_image = X_test[i].reshape(1, 28, 28, 1)\n",
    "\n",
    "    #Perform forward pass through the convolutional, ReLU, and max pooling layers\n",
    "    pooled_output = model.pool.forward(model.relu.forward(model.conv.forward(test_image)))\n",
    "\n",
    "    # Flatten the output of the pooling layer\n",
    "    flattened_output = pooled_output.reshape(pooled_output.shape[0], -1)\n",
    "\n",
    "    #Perform forward pass through the fully connected layer to get logits\n",
    "    logits = model.fc.forward(flattened_output)\n",
    "    #Compute the predicted class (index with the highest logit score)\n",
    "    prediction = np.argmax(logits)\n",
    "    #Get the true label from the one-hot encoded target vector\n",
    "    true_label = np.argmax(y_test[i])\n",
    "    #Check if the prediction matches the true label\n",
    "    if prediction == true_label:\n",
    "        correct_predictions += 1\n",
    "\n",
    "#Calculate accuracy by dividing the number of correct predictions by the total number of test samples\n",
    "accuracy = correct_predictions / len(X_test)\n",
    "#Print the test accuracy percentage\n",
    "print(f\"Test accuracy: {accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Af3UlMvDEoIU"
   },
   "source": [
    "# Conclusion\n",
    "- In this implementation, a Convolutional Neural Network (CNN) model for classifying handwritten digits from the MNIST dataset was developed and trained. \n",
    "- The model architecture consists of a convolutional layer followed by ReLU activation, max-pooling, and a fully connected layer with softmax activation for multiclass classification.\n",
    "- The model was trained using the Stochastic Gradient Descent (SGD) optimizer over 5 epochs with a batch size of 250.\n",
    "### Training and testing of the model\n",
    "- During training, the model learned to recognize intricate patterns and features in the input images, enabling it to make accurate predictions.\n",
    "- The training process involved iterative forward and backward passes, adjusting the model's parameters to minimize the softmax cross-entropy loss.\n",
    "- After the training process, the model demonstrated a high level of accuracy on the unseen test dataset, achieving an accuracy of 96.44%.\n",
    "\n",
    "<b>This high test accuracy indicates the effectiveness of the CNN architecture and the optimization techniques used.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
